{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import factorial, softmax\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buffet import IndianBuffet\n",
    "\n",
    "def init_ibm(data):\n",
    "    Xs = len(data)\n",
    "    return IndianBuffet(data, (5, 1, 1), [(1.7, 1, 1)] * Xs, [(0.5, 1, 1)] * Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = get_data(include_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm = init_ibm(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibm.run_sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, Xs):\n",
    "    best = (-float('inf'), None, None)\n",
    "    for Z in model.Zs:\n",
    "        m = Z.sum(axis=0) - Z\n",
    "        lps = np.sum(np.log(np.where(Z == 1, m, model.N - m) / model.N), axis=1)\n",
    "        for f, X in zip(model.feats[:-1], Xs[:-1]):\n",
    "            if X is not None:\n",
    "                X_bar = Z @ f.weights(Z)\n",
    "                diffs = np.sum((X - X_bar) ** 2, axis=1) / (2 * f.var_x)\n",
    "                lps -= diffs + np.log(2 * np.pi) / 2 + np.log(f.var_x) * f.D / 2\n",
    "        i = np.argmax(lps)\n",
    "        if lps[i] > best[0]:\n",
    "            best = lps[i], Z, i\n",
    "    best_lp, best_Z, best_i = best\n",
    "    W = model.feats[-1].weights(best_Z)\n",
    "    X_w_pred = best_Z[best_i] @ W\n",
    "    \n",
    "    return Xs[-1][np.argmax(X_w_pred)] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_types(X):\n",
    "    N = len(X[0])\n",
    "    return {\n",
    "        'multi': X,\n",
    "        'vision_uni': [X[0], [None] * N, X[2]],\n",
    "        'audio_uni': [[None] * N, X[1], X[2]],\n",
    "        'vision': [X[0], X[2]],\n",
    "        'audio': [X[1], X[2]],\n",
    "    }\n",
    "\n",
    "def run_experiment(X_train, X_test, seed=None):\n",
    "    N_train = len(X_train[0])\n",
    "    N_test = len(X_test[0])\n",
    "    trains = get_X_types(X_train)\n",
    "    tests = get_X_types(X_test)\n",
    "    \n",
    "    train_test_map = {\n",
    "        'vision': ['vision'],\n",
    "        'audio': ['audio'],\n",
    "        'multi': ['multi', 'vision_uni', 'audio_uni']\n",
    "    }\n",
    "    ibms = []\n",
    "    \n",
    "    for train_type in train_test_map.keys():\n",
    "        np.random.seed(seed)\n",
    "        print(f'Training {train_type}...')\n",
    "        ibm = init_ibm(trains[train_type])\n",
    "        ibm.run_sampler(iters=5000)\n",
    "        avg_K = np.mean([Z.shape[1] for Z in ibm.Zs])\n",
    "        print(f'Average K: {avg_K}')\n",
    "        \n",
    "        for test_type in train_test_map[train_type]:\n",
    "            train_predictions = [predict(ibm, xs) for xs in zip(*trains[test_type])]\n",
    "            test_predictions = [predict(ibm, xs) for xs in zip(*tests[test_type])]\n",
    "            \n",
    "            train_acc = sum(train_predictions) / N_train\n",
    "            test_acc = sum(test_predictions) / N_test\n",
    "            \n",
    "            print(f'TRAIN: {train_type}, TEST: {test_type}', f'train accuracy: {train_acc}', f'test_accuracy: {test_acc}', sep='\\n')\n",
    "        ibms.append(ibm)\n",
    "    return ibms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(X_train, X_test, seed=7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Bhattacharyya distance between p(Z|X_V) & p(Z|X_A)\n",
    "def batt_dist(model):\n",
    "    dist = 0\n",
    "    posts_A = softmax(model.prior() + np.array([model.feats[1].log_likelihood(Z) for Z in model.Zs]))\n",
    "    posts_V = softmax(model.prior() + np.array([model.feats[0].log_likelihood(Z) for Z in model.Zs]))\n",
    "\n",
    "    pq = posts_A * posts_V\n",
    "    dist = np.sum(np.sqrt(pq))\n",
    "\n",
    "    return - np.log(dist)\n",
    "\n",
    "# Calculate Bhattacharyya distance between p(Z|X_V) & uniform\n",
    "def batt_dist_unif(model):\n",
    "    dist = 0\n",
    "    \n",
    "    posts_V = softmax(model.prior() + np.array([model.feats[0].log_likelihood(Z) for Z in model.Zs]))\n",
    "    \n",
    "    pq = posts_V / len(model.Zs)\n",
    "    dist = np.sum(np.sqrt(pq))\n",
    "\n",
    "    return - np.log(dist)\n",
    "\n",
    "print(batt_dist(ibm))\n",
    "print(batt_dist_unif(ibm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
